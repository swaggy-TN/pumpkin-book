#MachineLearning
##Chapter 1
这一章节以现实生活中的例子来引入，介绍了很多learning theory中的基本术语，
没太多值得记录的，将具体问题抽象化，给出数学formulation，总的来说形象但不够形象
一个新知识点：NFL定理--在函数空间中对所有可能对f按均匀分布对假设对误差求和，最后结论发现误差期望与学习算法无关。
然而我的理解是，既然已经在整个函数空间中进行考虑，且有均匀分布对假设，那这着实与学习器无关啊，可以视作一个
完全没有学习过的模型，那么其误差期望和（二分类的话）和掷骰子一样啊。。。。

还介绍了一些机器学习相关的领域，当然了，我个人觉得统计学习和机器学习的关系更大一些，我也不是很能tell difference，
但是确实当一切涉及到深度学习的时候，数据量和模型capacity变成了整个learning过程那个dominant的点，机器学习的算法
据我了解最大的优势是其latency很低。

##Chapter2

这一张模型的评估与选择，收获还是比较大的：

###2.5
泛化误差=偏差bias+方差var+noise的证明很详实，确实和直觉相符合，毕竟真实label很可能和dataset
label是有出入的，标数据集也有人工的误差的，印象中NAACL2021中有篇paper就在做这个事，
QA相关的很多dataset都是有train test overlap或label misleading的问题；

###2.4
后几个检验机制没有看懂 #TODO: 比较检验的p40-44仅粗略看了下，没有细推
不过对于统计假设检验的描述很有意思，"若在测试集上观察到学习器A比B好，则A的泛化能力是否在统计意义上优于B"
在dev上的best_checkpoint一般默认拿来跑test，或者paper中有时候corpus leaderboard的test label不公布，
则直接拿dev的结果做比较，虽然横向比较其他baseline的话也不能说有失公平，但是确实，一个在dev上过拟合的model，
如何证明是真的generalized model，而不是刚好这个model收敛在了一个能过拟合dev的局部最优（？）的点呢。

###2.3
介绍了一些常规metrics，accuracy、precision、recall以及combine起来的F1-score，这部分没什么可说的

ROC和AUC却是第一次接触，其中给出了很多变量缩写，但个人感觉用rate反而不如用recall来表示更容易让人理解
而且（这里完全是个人理解，或许有错）ROC曲线的画法其实是很奇怪的，真正training时我们更关心最终的label，也就是阈值
0.5以上的部分，即，图像左侧的斜率越高越好，尤其是对于QA，retrieval R@1和R@5应该是最难但是最重要的，毕竟R@100大家
指标都大差不差，well, it depends...

当然这个AUC的面积有时候确实可以用来衡量两个学习器的优劣，看需要的是什么了，暂时能想到的是distill中的teacher model
可能需要soft label中富含更多的分布信息，此时一个很强的产出类似hard label的分布的模型可能就不是很适合了。

代价敏感错误率与代价曲线这个subsection中，个人对于cost-sensitive的理解是，此时loss不只是和模型产出的P有关，
不同的label有不同的weight，那么模型就会有不同的优化方向

代价曲线和期望总体代价这部分详情查询https://www.zhihu.com/question/63492375/answer/247885093
高亮1、2条写得很好

纵轴可以视作错误率，横轴是正例的加权后的概率，那换句话说，这个图就是在描述当样本空间中的正例的占比不同时，错误率的期望
都是多少，那么这个面积的部分代表了，全空间不管样本怎么变，他整个的错误率的（加权）期望

###2.1 和 2.2
没什么好说的，明确了经验误差和真实误差，给出了过拟合欠拟合的概念，教你怎么切dev test train，有什么方法能更好的让
模型在数据量不充足的情况下利用好现有数据，获得的信息量最丰富获得最general的model。



