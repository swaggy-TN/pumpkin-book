#Chapter4
##4.1基本流程

这一节简单介绍了决策树，给出了决策树返回递归的伪算法，简单来说就是最后分无可分，无论是基于属性还是基于
剩余的样本集是否为空

##4.2划分选择
###4.2.1信息增益gain
再次给出信息熵的概念，以离散属性为例给出information gain的概念，表达
的是当做出基于某个属性的某个值的划分时，信息熵的减小程度。表达的意思和互信息有类似之处，
只不过互信息是联合分布与y关于x的条件概率的KL散度
###4.2.2增益率 gain ratio
很明显这相当于做一个正则化，避免决策过程是基于类似编号这种样本数量极少且能带来极大信息增益的属性来划分的
正则项是一个分母上的信息熵，也就是当前的这个属性含样本量越大熵越大
但是这就是我不太能get到的一点了，当属性的样本量小的时候，正则项是鼓励的，那岂不是很容易就按照编号来分
我个人能想到的对策就是直接把编号这种扯淡的属性从属性集中移除
###4.2.3基尼系数
另一个很显式的标准，标记不一样的概率，因为概率都是小于1且求和为1的，所以很显然，越平均，其值越大
所以我们的方向是使得Gini的值越小越好
##4.3剪枝
分pre-pruning和post-pruning,很直观，pre-pruning是在生成树的时候，每到一个node都要做一次，
所以直观上来说，树的深度和广度都会比较低
post-pruning是生成完了之后从每个leaf开始回头检查的，这很合理，但是会time consuming
##4.4连续与缺失值
###4.4.1连续值处理
很简单，对于连续属性a，可以对所有样本的a值做插值获得一个个的点，然后当作离散值处理
但是这很蠢，当样本量多的时候，可以想像时间空间复杂度都会是很恐怖的
###4.4.2缺失值处理
1）属性值缺失的情况下进行划分属性的选择

当有样本缺失属性值当时候，我们总结没有缺失的样本，为另一个集合，同时基于统计概率给出三个权重
一个是rou=不缺的/整体，一个是p=每个标签集下的不缺的/标签集下的完整的，和r=一个属性当前值内不缺的/一个属性当前值下完整的
然后在gain的公式中相关项中乘上这些权重

2）给定划分属性，若样本在该属性的值缺失了，如何对样本进行划分

更直观了，根据当前属性不同值的样本的概率来分配这个缺失值的样本在不同分支的权重

##4.5多变量决策树
其实就是多条件呗，很显然真实分类中很可能边界是模糊的，线性的不如hyperbolic的，所以在分类决策上也可以
做个线性加权的多属性决策，使得分类边界不是横平竖直，而是一些斜线。