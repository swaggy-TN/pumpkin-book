#Chapter 3
##3.1基本形式

给出了lenear model 的math formulation没什么好说的

##3.2线性回归(linear regression)

given labeled data,我们可以通过线性回归的方式去拟合它的概率分布，也就是，当再给出一个data时，我们通过解出来的线性
模型可以预测它的label。那么问题就变成了如何去学习这个线性模型，答案即通过线性回归的方式。线性回归，就是去找一条线（或者超平面）
，使得空间中的数据点，到这条线（超平面）的欧式距离最短。所以我们就得到了一个argmin问题，通过求解可以得到omega和b
而多元和一元线性回归的区别仅仅在于特征的个数，和求偏导时的矩阵求导规则不同，本质毫无差别

##3.3对数几率回归(logistic regression)

对数几率回归就是讨巧，通过回归的方式做分类，具体的，几率就是真/非真，对数几率就是说，当真/非真>1也就是模型更倾向于认为当前数据点
是真的时候，ln(y/1-y)会大于0，我们通过这样一个sigmoid函数来完成二分类的预测

最后的MLE来最大对数似然，推完结果其实就是二分类的cross-entropy函数，这也很符合torch的实现

##3.4线性判别分析(LDA:linear discriminant analysis)

LDA的motivation其实是很直观的，找一个超平面，整个数据集在这个超平面上的投影满足“类内近，类间远”的分布。那么现在
如何找这样最优的平面，使得类内方差最小，且类间距离尽可能大呢，我们记J = 类内/类间 = WSbW/WSwW，现在的优化目标就是求
使得J取最大值的W，通过拉格朗日乘子法（f为凸函数，hi为一堆f的约束条件，f和hi的求和记为L，通过对L求偏导，得到的导函数零点，即f的极值点）
我们就可以获得这样一个最优的W

##3.5多分类学习

多分类学习这一章在线性模型给出其实还挺奇怪的，因为线性模型只能学到一条直线，想用一条直线做多分类，直观上就很麻烦，无非就
是做多次二分类来获得最终类别的prediction

ECOC编码说实话有点扯淡，当然书中也提到了在码长较小的时候会有效，长度越长越差，这个技术不做表示

##3.6类别不平衡问题

这个问题非常常见了多数情况下会用负采样，毕竟labeled data很难得，能多用最好别少用，再缩放rescaling这个点倒是很有意思的
就实际问题而言，就相当于是train完（还是train中？）然后把正例的label改为比如0.4，那么大于0.2即视为正例